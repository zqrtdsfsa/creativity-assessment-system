# 学生创造力测评与评分说明

本文档详细说明本系统的测评流程、题库来源、评分维度与算法、并行双Agent策略、日志留存与可配置项，便于研究复现与运维排查。

## 1. 测评流程（逐题答题-逐题评分）

- 进入“学生测评”页，录入学生信息并开始测评。
- 系统从本地题库中抽取若干题目（默认 `MAX_QUESTIONS`），逐题展示：
  - 学生在输入框提交答案后，系统立即对该题进行评分，并进入下一题。
- 全部题目结束后，系统按四个维度（流畅性、灵活性、独创性、精细性）计算平均分与总分，并保存测评结果。
- 在“结果分析”页可：
  - 选择学生与具体某一次测评结果查看详情；
  - 如有多次历史，查看按“测评次数”均匀分布的趋势图。

## 2. 题库与出题策略

- 题库位于 `questions/` 目录，按类型分为四个 JSON 文件：
  - `divergent_thinking.json`（发散思维）
  - `convergent_thinking.json`（聚合思维）
  - `creative_problem_solving.json`（创造性问题解决）
  - `imagination.json`（想象力）
- 首次运行时，系统会自动为每类生成 100 道题（可直接编辑这些 JSON 进行自定义）。
- 出题时从上述文件中随机抽取，尽量保证类型分布均衡；不足时会跨类型补齐，并最终随机打乱顺序。

## 3. 评分维度与含义

- 流畅性（Fluency）：答案数量与丰富程度
- 灵活性（Flexibility）：视角多样性与变化性
- 独创性（Originality）：独特性与创新性
- 精细性（Elaboration）：细节完整度与深度

每个维度分值范围为 0–10 分。总分为四个维度之和（0–40）。

## 4. 评分算法（并行双Agent取平均）

- 系统使用两套大模型 Agent 并行打分：
  - AgentA（偏保守，温度 0）
  - AgentB（偏发散，温度 0.3）
- 两个 Agent 的提示词具有相同格式，但带不同的“角色倾向”引导。
- 模型返回的 JSON 解析后，四维分数按算术平均合并；两者之一失败时采用另一个；两者都失败时使用默认分数（7 分/维）。
- 最终在结果页展示维度分、总分，以及合并备注（会包含两者的 `comments` 简要拼接）。

### 4.1 JSON 解析稳健性

- 模型返回常见情况：含有说明文字、` ```json ` 代码块包裹、或前后缀文本。
- 系统会自动提取第一个花括号 JSON 片段解析；解析失败则回退默认分并在日志中记录原文。

## 5. 日志与可观测性

- 日志目录：`log/`
  - `app.log`：前端交互与数据库关键事件（开始测评、保存答案、评分完成、保存结果等）
  - `llm.log`：LLM 交互日志（PromptA/PromptB、原始返回、解析结果、异常堆栈）
- 日志按天轮转，默认保留 7 天。

## 6. 超时、重试与并发

- 默认为：
  - AgentA：`timeout=15s`，`max_retries=0`（保证快速反馈）
  - AgentB：`timeout=15s`（可调整更长，如 45s），`max_retries=0`（可按需提高）
- 建议：若网络不稳或模型端拥堵，可适度提高 AgentB 的超时与重试；也可将“先返回者即用，另一个忽略”作为策略（当前为并行等待两者，使用平均/回退逻辑）。

## 7. 可配置项（环境变量）

- 模型与并行：
  - `SILICONFLOW_API_KEY`：模型 Key
  - `SILICONFLOW_BASE_URL`：服务地址，例如 `https://api.siliconflow.cn/v1`
  - `SILICONFLOW_MODEL_CHAT`：单模型名（两 Agent 未单独配置时使用）
  - `SILICONFLOW_MODEL_CHAT_A`、`SILICONFLOW_MODEL_CHAT_B`：双Agent分别使用的模型名
- 题量与时间：
  - `MAX_QUESTIONS`：本次测评题目数量（默认 10，可在 `.env` 设置）
  - `TIME_LIMIT_MINUTES`：测评时长显示（用于界面提示）
- 数据库：
  - `DATABASE_URL`：默认 `sqlite:///./creativity_assessment.db`
- 其他：
  - `DEBUG`：调试模式（True/False）

> 提示：在 Windows PowerShell 中临时设置示例：
>
> ```powershell
> $env:SILICONFLOW_API_KEY="你的key"
> $env:SILICONFLOW_BASE_URL="https://api.siliconflow.cn/v1"
> $env:SILICONFLOW_MODEL_CHAT="DeepSeek-V3"
> $env:MAX_QUESTIONS="3"
> ```

## 8. 结果展示与图表

- 雷达图：显示四维分并已针对中文标签优化边距与角轴配置。
- 柱状图：显示各维度分数，按照分数等级变色。
- 趋势图：横坐标为“测评次数”（第 1 次、2 次……），悬浮提示展示具体日期与分数。

## 9. 常见问题（FAQ）

- Q：页面长时间“正在评分”？
  - A：可能网络或模型端响应慢。按需提高 AgentB 超时或降低题量 `MAX_QUESTIONS`；查看 `log/llm.log`。
- Q：LLM 返回非 JSON 导致解析失败？
  - A：系统已做容错，仍失败则使用默认分并记录原文到 `llm.log` 以便优化提示词。
- Q：同一学号重复创建报唯一约束？
  - A：使用已有档案或在数据库里删除旧记录；也可将创建逻辑改为 UPSERT/忽略重复。
- Q：如何扩充/修改题库？
  - A：直接编辑 `questions/*.json`，结构包含 `id/type/title/content/time_limit/dimensions/scoring_criteria` 字段。

## 10. 复现与审计建议

- 固定模型与超时/重试参数，记录题目与答案快照（可在 `app.log` 记录题目ID与答案长度；如需全量落库请注意隐私合规）。
- 导出测评结果 CSV 以便统计分析与外部审查。

---

如需进一步细化维度评分的“评分细则（Rubric）”，可在提示词中加入量化标准，例如为每个维度提供 0/3/6/8/10 的分档描述，并在 `question_bank.py` 中为特定题型附带针对性评分要点，以提升一致性与可解释性。
